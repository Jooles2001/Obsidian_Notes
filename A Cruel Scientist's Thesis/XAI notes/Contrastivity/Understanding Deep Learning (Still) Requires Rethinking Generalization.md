Paper: https://dl.acm.org/doi/pdf/10.1145/3446776

Remarks:
- Deep neural networks easily fit random labels.
- Explicit regularization may improve generalization performance, but is neither necessary nor by itself sufficient for controlling generalization error.
---
> **Theorem:** There exists a two-layer neural network with `ReLU` activations and $2n + d$ weights that can represent any function on a sample of size $n$ in $d$ dimensions.