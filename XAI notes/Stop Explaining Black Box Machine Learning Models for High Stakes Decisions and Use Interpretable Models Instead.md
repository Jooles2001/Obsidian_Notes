*Cynthia Rudin - Duke University*
https://arxiv.org/pdf/1811.10154.pdf


Myth : there exists a trade-off between accuracy and interpretability
--> When training different ML models, interpretable models allow for better understanding of the data and allow for iterative better data-processing, thus overcoming the little difference with stronger opaque ones.

Explanations are not faithful to the original model, else they would be equal to the original model, and therefore unnecessary.

Explanations sometimes do not leave enough information for it to make sense to a user.
--> saliency map that indicates only where the AI model is "looking"


#### Challenges 
* Constructing optimal logical models
* Constructing optimal sparse scoring systems
* Defining interpretability for specific domains and creating methods accordingly


#### Related articles
*This Looks Like That
--> This Does Not Look Like That
Explaining Explanations of AI
Deep Learning for Case-Based Reasoning through Prototypes*
